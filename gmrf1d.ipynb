{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.distributions as D\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt, log\n",
    "\n",
    "from sksparse.cholmod import cholesky\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from pyvi.utils import GMMLossFunc as lf\n",
    "from pyvi.utils import HelperFunc as hf\n",
    "\n",
    "\n",
    "device = (\"cuda\" if torch.cuda.is_available()\n",
    "          else \"mps\" if torch.backends.mps.is_available()\n",
    "          else \"cpu\")\n",
    "\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"Helvetica\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Gaussian Process with Matérn $\\nu = 1/2$ Covariance Function\n",
    "\n",
    "### 1. Matérn kernel\n",
    "\n",
    "The Matérn covariance function is given by\n",
    "$$c_{\\nu}(r; \\sigma, \\rho) = \\sigma^2 \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left( \\sqrt{8\\nu} \\frac{r}{\\rho} \\right)^\\nu K_{\\nu}\\left( \\sqrt{8\\nu} \\frac{r}{\\rho} \\right)$$\n",
    "\n",
    "When $\\nu = 1/2$, this simplifies to:\n",
    "$$c_{1/2}(r; \\sigma, \\rho) = \\sigma^2 \\exp\\left( - \\frac{2r}{\\rho} \\right)$$\n",
    "\n",
    "We use the following parameterization from Fuglstad et al. (2018):\n",
    "\n",
    "$$ \\kappa = \\sqrt{8\\nu} / \\rho \\qquad \\text{and} \\qquad \\tau = \\sigma \\kappa^\\nu \\sqrt{\\frac{\\Gamma(\\nu + d/2)(4\\pi)^{d/2}}{\\Gamma(\\nu)}},$$\n",
    "where $d$ is the dimension of the state-space.\n",
    "\n",
    "For fixed $\\nu$, the reciprocal is given by:\n",
    "\n",
    "$$ \\rho =  \\sqrt{8\\nu} / \\kappa  \\qquad \\text{and} \\qquad \\sigma =  \\tau \\kappa^{-\\nu} \\sqrt{\\frac{\\Gamma(\\nu)}{\\Gamma(\\nu + d/2)(4\\pi)^{d/2}}}$$\n",
    "\n",
    "When $d = 1$ and $\\nu =1/2$,   $\\sqrt{\\frac{\\Gamma(\\nu + d/2)(4\\pi)^{d/2}}{\\Gamma(\\nu)}} = \\sqrt{2}$. It follows that $$\\kappa = 2/ \\rho \\quad \\& \\quad\\tau = \\sigma \\sqrt{2\\kappa} $$\n",
    "\n",
    "### 2. PC Prior\n",
    "\n",
    "\n",
    "* The PC prior for $\\tau$ with base model $\\tau =0$ is $$\\tau|\\kappa \\sim \\text{Exp}(\\lambda),$$ with $$\\lambda(\\kappa) = - \\kappa^{-\\nu} \\sqrt{\\frac{\\Gamma(\\nu)}{\\Gamma(\\nu + d/2)(4\\pi)^{d/2}}} \\frac{\\log(\\alpha)}{\\sigma_0}$$ so that $P(\\sigma > \\sigma_0|\\kappa) = \\alpha$. \n",
    "For $d = 1$ and $\\nu =1/2$, $$\\lambda(\\kappa) = - \\frac{1}{\\sqrt{2\\kappa}}\\frac{\\log(\\alpha)}{\\sigma_0}$$\n",
    "\n",
    "* The PC prior for $\\kappa$ with base model $\\kappa=0$ is $$\\kappa \\sim \\text{Weibull}(\\lambda^{-2/d}, d/2),$$ which satifies $P(\\rho <\\rho_0) = \\alpha$ if $$ \\lambda = - \\left(\\frac{\\rho_0}{\\sqrt{8\\nu}} \\right)^{d/2} \\log(\\alpha).$$ For  $d = 1$ and $\\nu =1/2$, this simplifies to \n",
    "    $$\\kappa \\sim \\text{Weibull}(\\lambda^{-2}, 1/2), \\qquad \\& \\qquad \\lambda = - \\sqrt{\\frac{\\rho_0}{2}} \\log(\\alpha)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================================================\n",
    "# define the Weibul-Exponential PC prior for the GMRF hyperparameters\n",
    "#============================================================================================\n",
    "\n",
    "class PriorPC():\n",
    "    '''\n",
    "        Defines Penalized Complexity (PC) prior class over the hyperparameters `sigma` and `rho` of a GMRF with Matern covariance with smoothness nu=1/2.\n",
    "\n",
    "        The PC prior is derived using the reparameterization `kappa = 2/rho` and `tau = sigma * sqrt{2 kappa}`. We use this parameterization\n",
    "        and transform back samples to the `sigma-rho` parameterization.  \n",
    "\n",
    "        The PC prior sets an exponential distribution on `tau` conditionnal on `kappa` and a Weibul distribution on `kappa`.\n",
    "            y ~ N(mu, sigma2)\n",
    "            where both `mu` and `sigma2` are unknown \n",
    "\n",
    "        For details about the derivation of the PC prior and a general formulation for any value of the smoothness parameter `nu`, see\n",
    "\n",
    "        Geir-Arne Fuglstad, Daniel Simpson, Finn Lindgren & Håvard Rue (2019) Constructing Priors that Penalize the Complexity of Gaussian Random Fields,\n",
    "          Journal of the American Statistical Association, 114:525, 445-452, DOI: 10.1080/01621459.2017.1415907\n",
    "\n",
    "        The paper can be accessed here: https://doi.org/10.1080/01621459.2017.1415907\n",
    "    '''\n",
    "    def __init__(self, rho_0, sigma_0, alpha_rho=0.05, alpha_sigma=0.05):\n",
    "        # recode hyperparameters. This allows for the object to be updated with no need of re-instantiation\n",
    "        self.params = {'rho_0':rho_0, 'sigma_0':sigma_0, 'alpha_rho':alpha_rho, 'alpha_sigma':alpha_sigma}\n",
    "        \n",
    "        # define the Weibull distribution on `kappa`\n",
    "        lambda_ = -sqrt(0.5 * self.params['rho_0']) * log(self.params['alpha_rho'])\n",
    "        self.kappa = D.Weibull(1 / lambda_**2, torch.tensor([0.5]))\n",
    "        \n",
    "        # define the conditional exponential distribution for `tau` given  `kappa`\n",
    "        self.tau = lambda kappa: D.Exponential(- torch.log(torch.tensor([self.params['alpha_sigma']])) / (torch.sqrt(2 * kappa) * self.params['sigma_0']))\n",
    "        \n",
    "    def sample(self, shape: torch.Size = ()):\n",
    "        '''\n",
    "        Method for sampling `(rho, sigma)` from the PC prior distribution. This is 3-step approach:\n",
    "            (1) sample: kappa ~ Weibull(lambda, 1/2)\n",
    "            (2) sample: tau|kappa ~ Exp(lambda(kappa))\n",
    "            (3) map (kappa, tau) ---> (rho, sigma)\n",
    "\n",
    "        '''\n",
    "        kappa = self.kappa.sample(shape).squeeze()                # (1) sample: kappa ~ Weibull(lambda, 1/2)\n",
    "        #kappa = torch.ones(shape) * 2.0\n",
    "        tau = self.tau(kappa).sample()                            # (2) sample: tau|kappa ~ Exp(lambda(kappa))\n",
    "\n",
    "        # (3) map (kappa, tau) ---> (rho, sigma)\n",
    "        rho = 2 / kappa\n",
    "        sigma = tau / torch.sqrt(2 * kappa)\n",
    " \n",
    "        theta = torch.stack([rho, sigma], dim=-1)       # combine\n",
    "        return theta.squeeze(), kappa, tau\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = PriorPC(rho_0=.1, sigma_0=10.0, alpha_rho=0.05, alpha_sigma=0.05)\n",
    "theta, kappa, tau =  prior.sample((10000,))\n",
    "\n",
    "fig = plt.figure(figsize=(12, 3), dpi=200)\n",
    "\n",
    "fig.add_subplot(141)\n",
    "plt.hist(kappa.log(), bins=25, density=True)\n",
    "plt.title(r'$\\log(\\kappa)$')\n",
    "\n",
    "fig.add_subplot(142)\n",
    "plt.hist(tau.log(), bins=25, density=True)\n",
    "plt.title(r'$\\log(\\tau) $')\n",
    "\n",
    "fig.add_subplot(143)\n",
    "plt.hist(theta[...,0].log(), bins=25, density=True)\n",
    "plt.title(r'$\\log(\\rho)$')\n",
    "\n",
    "fig.add_subplot(144)\n",
    "plt.hist(theta[...,1].log(), bins=25, density=True)\n",
    "plt.title(r'$\\log(\\sigma)$')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Gaussian Process with Matérn Covariance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaternGP(theta, r, n, n_sample=1):\n",
    "    '''\n",
    "        Input:\n",
    "            - theta = [rho, sigma2]  (tensor), hyperparameters of the covariance function\n",
    "            - r: resolution, distance between 2 successive points on the grid\n",
    "            - n: number of points on the grid (regular)\n",
    "            - n_sample: number of samples to draw from the GP\n",
    "    '''\n",
    "    rho, sigma  = theta.tolist()\n",
    "    sigma2 = sigma ** 2\n",
    "    #########################################\n",
    "    # define the Matern precision matrix\n",
    "    ########################################\n",
    "    scale = np.exp(- 2 * r / rho) / (sigma2**2 * (1 - np.exp(- 4 * r / rho)))      # scaling factor\n",
    "    off_diag = - np.ones(n)                                                       # off diagonals elements\n",
    "    diag = [np.exp(2 * r / rho)] + [np.exp(2 * r / rho) + np.exp(-2 * r / rho)] * (n-2) + [np.exp(2 * r / rho)]   # diagonal elements\n",
    "    \n",
    "    data = scale * np.array([off_diag, diag, off_diag])       # put off diagonal and diagonal elements in the same array\n",
    "    offsets = np.array([-1, 0, 1])        \n",
    "\n",
    "    Q = sp.dia_matrix((data, offsets), shape=(n,n)).tocsc()          # define the precision matrix as a sparse matrix\n",
    "\n",
    "    #########################################\n",
    "    # sampling from the GP\n",
    "    ########################################\n",
    "    # compute Cholesky factorization\n",
    "    factor = cholesky(Q)\n",
    "    \n",
    "    # sample z ~ N(0, I)\n",
    "    z = np.random.randn(n, n_sample)\n",
    "\n",
    "    # solve L.T @ v = z\n",
    "    v = factor.solve_Lt(z)\n",
    "\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulator(theta, r, n, n_sample=1):\n",
    "    Y = np.zeros((theta.shape[0], n, n_sample))\n",
    "\n",
    "    for i in range(theta.shape[0]):\n",
    "        y = MaternGP(theta[i], r, n, n_sample)\n",
    "        Y[i] = y \n",
    "    \n",
    "    Y = torch.from_numpy(Y).to(torch.float32)\n",
    "    \n",
    "    return Y.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = simulator(theta[:10,:], r=0.1, n=1001, n_sample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = MaternGP(torch.tensor([100.0, 1.0]), 0.01, 10000, 1)\n",
    "\n",
    "k = np.random.randint(Y.shape[0])\n",
    "text = r'$\\rho = $' + str(round(theta[k].numpy()[0], 1)) + ',  ' +  r'$\\sigma = $' + str(round(theta[k].numpy()[1], 1))\n",
    "y = Y[k]\n",
    "x = np.linspace(0.0, 100.1, 1001, endpoint=False)\n",
    "\n",
    "plt.plot(x, y, lw=1.0)\n",
    "plt.title(text)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "     '''\n",
    "        Multivariate Gaussian Mixture Density Network class for amortized variational inference\n",
    "\n",
    "        Gaussian mixtures are dense in the space of probability distributions. This motivates their use for posterior density approximation.\n",
    "\n",
    "        Each mixture is parameterized by the means, Cholesky factors of associated precision matrices and mixture weights.\n",
    "\n",
    "        The neural network does not output the Cholesky factors directly but rather tensors containing their respective lower diagonal elements.\n",
    "    '''\n",
    "     def __init__(self, input_size, dim:int=2, K:int=1, hd:int=64):\n",
    "        '''\n",
    "            Input: \n",
    "                * input_size: dimension of the input to the neural net, i.e. the number of elements in the observation vector yobs\n",
    "                * dim:        dimension of the posterior distribution. This is in general the number of parameters in the model.\n",
    "                * K:          number of mixture components\n",
    "                * hd:     dimension of hidden layers\n",
    "            Output:\n",
    "                * mean: tensor of dimensions batchsize X k X dim containing the predicted means\n",
    "                * chol: tensor of appropriate dimensions containing the the predicted Cholesky factors\n",
    "                * coeff: tensor of appropriate dimensions containing the predicted mixture component weights\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.K = K\n",
    "        self.hd = hd\n",
    "        self.input_size = input_size \n",
    "\n",
    "\n",
    "        # convolutional layers\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(1, 1, 5),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool1d(3),\n",
    "            nn.LayerNorm((self.input_size - 4) // 3),\n",
    "            nn.Conv1d(1, 1, 5),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.LayerNorm((((self.input_size - 4) // 3) - 4) // 2)\n",
    "        )\n",
    "\n",
    "        # fully connected (((self.input_size - 4) // 3) - 4) // 2layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear((((self.input_size - 4) // 3) - 4) // 2, self.hd),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.hd, self.hd),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        \n",
    "        # means of each component\n",
    "        self.mean = nn.Linear(self.hd, self.K * self.dim)\n",
    "        \n",
    "        # Cholesky factors of the precision matrices (diagonal elements are log-scaled)\n",
    "        self.chol = nn.Linear(self.hd, self.K * self.dim * (self.dim + 1) // 2)\n",
    "\n",
    "        # mixture weights, non-negative and summing to 1\n",
    "        self.coeff = nn.Sequential(\n",
    "            nn.Linear(self.hd, self.K),\n",
    "            nn.Softmax(dim=1)   \n",
    "            )  \n",
    "\n",
    "        \n",
    "     def forward(self, x):\n",
    "        # apply convolutional layers\n",
    "        x = self.conv(x)\n",
    "        # flatten\n",
    "        x = x.flatten(1)\n",
    "        # apply fully-connected layers\n",
    "        x = self.fc(x)\n",
    "\n",
    "        # mean\n",
    "        mean = self.mean(x)\n",
    "        mean = mean.reshape((mean.shape[0], self.K, self.dim))\n",
    "\n",
    "        # Cholecky factor\n",
    "        chol = self.chol(x)\n",
    "        chol = chol.reshape((chol.shape[0], self.K, self.dim * (self.dim + 1) // 2))\n",
    "\n",
    "        # mixture weights\n",
    "        if self.K > 1:\n",
    "            coeff = self.coeff(x)\n",
    "        else:\n",
    "            coeff = torch.ones(1)\n",
    "        \n",
    "        return mean, chol, coeff\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explicitly provided seeds for training process\n",
    "random_seed = 12345\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "#========================================== \n",
    "#           generating training data\n",
    "#==========================================\n",
    "n_prior, n_obs, n_sample, r, batchsize = 10000, 100, 1, 0.1, 100\n",
    "\n",
    "# sample parameter values from the specified prior\n",
    "prior = PriorPC(rho_0=2.0, sigma_0=10.0, alpha_rho=0.5, alpha_sigma=0.05)\n",
    "Theta_train, kappa, tau =  prior.sample((n_prior,))\n",
    "\n",
    "# draw samples from the simulator conditioned on parameter values\n",
    "Y_train = simulator(Theta_train, r=r, n=n_obs, n_sample=n_sample)\n",
    "\n",
    "# put sigma and rho on log-scale for improved training\n",
    "Theta_train = Theta_train.log()\n",
    "\n",
    "# create a combined dataset and data loader\n",
    "data_train = torch.utils.data.TensorDataset(Y_train.unsqueeze(1), Theta_train)\n",
    "data_loader = torch.utils.data.dataloader.DataLoader(data_train, batch_size=batchsize, shuffle=True)\n",
    "\n",
    "\n",
    "#==================================================\n",
    "#  instantiate the Gaussian mixture net\n",
    "#=================================================\n",
    "#gmmnet = Net(input_size=n_obs, dim=2, K=2, hd=64).to(device)\n",
    "gmmnet = hf.MultivariateGaussianMDN(input_size=n_obs, dim=2, K=2, hd=128, sort=True).to(device)\n",
    "\n",
    "loss_fn = lf.GaussianMixtureLoss(aggr='mean')\n",
    "\n",
    "# # train DNN model\n",
    "gmmnet = hf.nn_optimizer(model=gmmnet, \n",
    "                       data_loader=data_loader,\n",
    "                       loss_fn=loss_fn,\n",
    "                       learning_rate=1e-4,\n",
    "                       eps=0.01, \n",
    "                       max_epochs=100,\n",
    "                       verbose=True,\n",
    "                       echo_after=1,\n",
    "                       path='trained_models/gmrf1d/gp_fc_notsorted.pt'\n",
    "                      )\n",
    "\n",
    "#gmmnet = torch.load('trained_models/gmrf1d/gp_rho_sigma_3.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation-based calibration check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sbc_gaussian(gmmnet, proposal, n_sim = 1e+4, ecdf=True, ecdf_diff=False, logscale=None):\n",
    "    '''\n",
    "    Perform simulation-based calibration check for a Gaussian mixture network for posterior approximation\n",
    "\n",
    "    Input:\n",
    "        -- gmmnet: Gaussian mixture network, with input size given by `sample_size`\n",
    "        -- proposal: proposal distribution `theta ~ p(theta)`, usually the same as the prior/proposal distribution used for training\n",
    "                     Note: must have a `sample()` method\n",
    "        \n",
    "        -- generator: function that takes parameter values `theta` as input and generate the corresponding simulator model\n",
    "                       `x ~ p(x|theta)`  as an instance of a class with a `sample` method\n",
    "        \n",
    "        -- sample_size: number of iid samples from `x ~ p(x|theta)` for each values of theta\n",
    "\n",
    "        -- n_sim: number of simulation from the joint distribution: theta ~ p(theta); x ~ p(x|theta)\n",
    "\n",
    "        -- ecdf: whether to output an eCDF or a histogram plot, default: ecdf=True\n",
    "\n",
    "        -- ecdf_diff: whether on the y-axis are the `ecdf(w)` values (if True) or `ecdf(w) - w` values (if False).\n",
    "                        This is ignored if ecdf=False.\n",
    "        \n",
    "        -- logscale: (iterable) contains dimensions of the model parameter vector `theta` that are on log-scale\n",
    "                        note: we use the standard Python counting, starting at 0\n",
    "\n",
    "    Note: 95% confidence intervals are based on the  Dvoretzky–Kiefer–Wolfowitz inequality (see https://en.wikipedia.org/wiki/Empirical_distribution_function, accessed: 20-05-2024)\n",
    "    \n",
    "    Output: SBC plot as a Pyplot figure\n",
    "    '''\n",
    "\n",
    "    # draw samples from the prior/proposal  theta ~ p(theta)\n",
    "    Theta = proposal.sample((n_sim,))[0]\n",
    "\n",
    "    # draw samples from the model x ~ p(x|theta)\n",
    "    X = simulator(Theta, r=r, n=n_obs, n_sample=n_sample).unsqueeze(1)\n",
    "    \n",
    "    # ensure all dimensions are on the right scale\n",
    "    if logscale:\n",
    "        for i in logscale:\n",
    "            Theta[:,i] = Theta[:,i].log()     # put sigma2 on logscale\n",
    "\n",
    "    \n",
    "    # run the gmmnet\n",
    "    with torch.no_grad():\n",
    "        mean, chol, coeff = gmmnet(X.to(device))\n",
    "\n",
    "    n_component, dim = mean.shape[1], mean.shape[2]\n",
    "\n",
    "    # calculate Cholesky factors\n",
    "    chol = torch.vmap(lf.exp_diagonal)(lf.to_triangular(chol.view(n_sim * n_component,  dim * (dim + 1) // 2), dim)).view(n_sim, n_component, dim, dim)\n",
    "\n",
    "    # caclulate precision matrices\n",
    "    precision = chol @ chol.transpose(2, 3)\n",
    "\n",
    "    # calculate covariance matrices\n",
    "    covariance = torch.linalg.inv(precision) \n",
    "\n",
    "    # define GMM variational marginal distributions and calculate cdf values for the true parameter values\n",
    "    W = torch.zeros((n_sim, dim))   # tensor of cdf values\n",
    "    \n",
    "    for j in range(dim):\n",
    "        # mixture weights\n",
    "        mix = D.Categorical(coeff)\n",
    "        # mixture components\n",
    "        comp = D.Normal(mean[:,:,j], torch.sqrt(covariance[:,:,j,j]))\n",
    "        # define the mixture\n",
    "        gmm = D.MixtureSameFamily(mix, comp)\n",
    "        # evaluate cdf\n",
    "        W[:,j] = gmm.cdf(Theta[:,j].to(device))\n",
    "\n",
    "\n",
    "    if ecdf:\n",
    "        #=====================================================\n",
    "        # ECDF plot\n",
    "        #=====================================================\n",
    "        fig = plt.figure(figsize=(8, 3), dpi=200)\n",
    "\n",
    "        # Calculate the empirical cumulative distribution function (ECDF)\n",
    "        eCDF = torch.arange(1, n_sim + 1) / n_sim\n",
    "\n",
    "        # calculate 95% confidence intervals for the eCDF\n",
    "        eps = np.sqrt(np.log(2 / 0.05) / (2 * n_sim))\n",
    "        eCDF_lower, eCDF_upper = eCDF - eps, eCDF + eps\n",
    "\n",
    "        # exact cdf\n",
    "        x = np.linspace(0, 1, 100)\n",
    "\n",
    "        # eCDF for mu\n",
    "        #===============\n",
    "        fig.add_subplot(121)\n",
    "\n",
    "        w = W[:,0].sort().values\n",
    "        if not ecdf_diff:\n",
    "            # plot eCDF and true CDF values\n",
    "            plt.step(w, eCDF, lw=1)\n",
    "            plt.plot(x, x, 'k--', lw=1)\n",
    "\n",
    "            # plot 95% confidence bands\n",
    "            plt.fill_between(w, eCDF_lower, eCDF_upper, color='red', alpha=0.2)\n",
    "\n",
    "            plt.ylabel(r'$F_{\\omega}$')\n",
    "        else:\n",
    "            plt.step(w, eCDF - w, lw=1)\n",
    "            #plt.fill_between(w, eCDF_lower - w, eCDF_upper - w, color='red', alpha=0.1)\n",
    "            plt.ylabel(r'$F_{\\omega} - \\omega$')\n",
    "\n",
    "        plt.xlabel(r'$\\omega$')\n",
    "        plt.title(r'$\\log(\\rho)$')\n",
    "\n",
    "        # eCDF plot for sigma2\n",
    "        #======================\n",
    "        fig.add_subplot(122)\n",
    "\n",
    "        w = W[:,1].sort().values\n",
    "        if not ecdf_diff:\n",
    "            plt.step(w, eCDF, lw=1)\n",
    "            plt.plot(x, x, 'k--', lw=1)\n",
    "            # plot 95% confidence bands\n",
    "            plt.fill_between(w, eCDF_lower, eCDF_upper, color='red', alpha=0.2)\n",
    "\n",
    "            plt.ylabel(r'$F_{\\omega}$')\n",
    "        else:\n",
    "            plt.step(w, eCDF - w, lw=1)\n",
    "            # plot 95% confidence bands\n",
    "            #plt.fill_between(w, eCDF_lower - w, eCDF_upper - w, color='red', alpha=0.1)\n",
    "            plt.ylabel(r'$F_{\\omega} - \\omega$')\n",
    "\n",
    "        plt.xlabel(r'$\\omega$')\n",
    "        plt.title(r'$\\log(\\sigma)$')\n",
    "\n",
    "        plt.tight_layout()\n",
    "    else:\n",
    "        #========================================\n",
    "        # plot histograms\n",
    "        #========================================\n",
    "        fig = plt.figure(figsize=(8, 3), dpi=200)\n",
    "\n",
    "        # mu\n",
    "        fig.add_subplot(1,2,1)\n",
    "        plt.hist(W[...,0], bins=20, density=True, alpha=.6, label=r'$F^{-1}(\\mu)$')\n",
    "        plt.title(r'$\\log(\\rho)$')\n",
    "        plt.legend(fontsize=7,  markerscale=.5)\n",
    "\n",
    "        # log(sigma2)\n",
    "        fig.add_subplot(1,2,2)\n",
    "        plt.hist(W[...,1], bins=20, density=True, alpha=.6, label=r'$F^{-1}(\\log(\\sigma^2))$')\n",
    "        #plt.title(r'$\\sigma^2$')\n",
    "        plt.legend(fontsize=7,  markerscale=.5)\n",
    "\n",
    "        plt.title(r'$\\log(\\sigma)$')\n",
    "        plt.legend(fontsize=7,  markerscale=.5)\n",
    "\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f = sbc_gaussian(gmmnet, prior, 1000, ecdf=True, ecdf_diff=False, logscale=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "posterior mean (+ credible interval) vs true value scatter plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = prior.sample((2,))[0]\n",
    "\n",
    "y = simulator(theta, r=r, n=n_obs, n_sample=n_sample).unsqueeze(1)\n",
    "\n",
    "# run the gmmnet\n",
    "with torch.no_grad():\n",
    "    mean, chol, coeff = gmmnet(y.to(device))\n",
    "\n",
    "n_sim, n_component, dim = mean.shape[0], mean.shape[1], mean.shape[2]\n",
    "\n",
    "# calculate Cholesky factors\n",
    "chol = torch.vmap(lf.exp_diagonal)(lf.to_triangular(chol.view(n_sim * n_component,  dim * (dim + 1) // 2), dim)).view(n_sim, n_component, dim, dim)\n",
    "\n",
    "# # caclulate precision matrices\n",
    "precision = chol @ chol.transpose(2, 3)\n",
    "\n",
    "# # calculate covariance matrices\n",
    "covariance = torch.linalg.inv(precision) \n",
    "\n",
    "# mixture weights\n",
    "mix = D.Categorical(coeff)\n",
    "# mixture components\n",
    "comp = D.MultivariateNormal(loc=mean, covariance_matrix=covariance)\n",
    "# define the mixture\n",
    "gmm = D.MixtureSameFamily(mix, comp)\n",
    "\n",
    "# draw samples from the approximate posterior\n",
    "N = 10000\n",
    "theta_post = gmm.sample((N,)).cpu().squeeze()\n",
    "\n",
    "# plotting\n",
    "fig = plt.figure(figsize=(12, 3), dpi=200)\n",
    "\n",
    "fig.add_subplot(141)\n",
    "plt.hist(theta_post[:,0,0], bins=100, density=True)\n",
    "plt.vlines(x=theta[0][0].log(), ymin=0, ymax=plt.axis()[-1], linestyles='dashed', colors='black', lw=1)\n",
    "plt.title(r'$\\log(\\rho)$')\n",
    "\n",
    "fig.add_subplot(142)\n",
    "plt.hist(theta_post[:,0,1], bins=100, density=True)\n",
    "plt.vlines(x=theta[0][1].log(), ymin=0, ymax=plt.axis()[-1], linestyles='dashed', colors='black', lw=1)\n",
    "plt.title(r'$\\log(\\sigma) $')\n",
    "\n",
    "fig.add_subplot(143)\n",
    "plt.hist(theta_post[:,1,0], bins=100, density=True)\n",
    "plt.vlines(x=theta[1][0].log(), ymin=0, ymax=plt.axis()[-1], linestyles='dashed', colors='black', lw=1)\n",
    "plt.title(r'$\\log(\\rho)$')\n",
    "\n",
    "fig.add_subplot(144)\n",
    "plt.hist(theta_post[:,1,1], bins=100, density=True)\n",
    "plt.vlines(x=theta[1][1].log(), ymin=0, ymax=plt.axis()[-1], linestyles='dashed', colors='black', lw=1)\n",
    "plt.title(r'$\\log(\\sigma) $')\n",
    "\n",
    "plt.suptitle('Gaussian mixture posterior')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = prior.sample((2,))[0]\n",
    "\n",
    "y = simulator(theta, r=r, n=n_obs, n_sample=n_sample).unsqueeze(1)\n",
    "\n",
    "# run the gmmnet\n",
    "with torch.no_grad():\n",
    "    mean, chol, coeff = gmmnet(y.to(device))\n",
    "\n",
    "n_sim, n_component, dim = mean.shape[0], mean.shape[1], mean.shape[2]\n",
    "\n",
    "# calculate Cholesky factors\n",
    "chol = torch.vmap(lf.exp_diagonal)(lf.to_triangular(chol.view(n_sim * n_component,  dim * (dim + 1) // 2), dim)).view(n_sim, n_component, dim, dim)\n",
    "\n",
    "# # caclulate precision matrices\n",
    "precision = chol @ chol.transpose(2, 3)\n",
    "\n",
    "# # calculate covariance matrices\n",
    "covariance = torch.linalg.inv(precision) \n",
    "\n",
    "# mixture weights\n",
    "mix = D.Categorical(coeff)\n",
    "# mixture components\n",
    "comp = D.MultivariateNormal(loc=mean, covariance_matrix=covariance)\n",
    "# define the mixture\n",
    "gmm = D.MixtureSameFamily(mix, comp)\n",
    "\n",
    "# draw samples from the approximate posterior\n",
    "N = 10000\n",
    "theta_post = gmm.sample((N,)).cpu().squeeze()\n",
    "\n",
    "# plotting\n",
    "fig = plt.figure(figsize=(12, 3), dpi=200)\n",
    "\n",
    "fig.add_subplot(141)\n",
    "plt.hist(theta_post[:,0,0], bins=25, density=True)\n",
    "plt.vlines(x=theta[0][0].log(), ymin=0, ymax=plt.axis()[-1], linestyles='dashed', colors='black', lw=1)\n",
    "plt.title(r'$\\log(\\rho)$')\n",
    "\n",
    "fig.add_subplot(142)\n",
    "plt.hist(theta_post[:,0,1], bins=25, density=True)\n",
    "plt.vlines(x=theta[0][1].log(), ymin=0, ymax=plt.axis()[-1], linestyles='dashed', colors='black', lw=1)\n",
    "plt.title(r'$\\log(\\sigma) $')\n",
    "\n",
    "fig.add_subplot(143)\n",
    "plt.hist(theta_post[:,1,0], bins=25, density=True)\n",
    "plt.vlines(x=theta[1][0].log(), ymin=0, ymax=plt.axis()[-1], linestyles='dashed', colors='black', lw=1)\n",
    "plt.title(r'$\\log(\\rho)$')\n",
    "\n",
    "fig.add_subplot(144)\n",
    "plt.hist(theta_post[:,1,1], bins=25, density=True)\n",
    "plt.vlines(x=theta[1][1].log(), ymin=0, ymax=plt.axis()[-1], linestyles='dashed', colors='black', lw=1)\n",
    "plt.title(r'$\\log(\\sigma) $')\n",
    "\n",
    "plt.suptitle('Gaussian mixture posterior')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstpaper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
